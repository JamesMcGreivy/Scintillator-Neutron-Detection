# -*- coding: utf-8 -*-
"""
Created on Thu Sep  3 12:21:00 2020

Based on code from
https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_08_4_bayesian_hyperparameter_opt.ipynb

@author: siefman1
"""

import pandas as pd
import os
import numpy as np
import time
import tensorflow.keras.initializers
import statistics
import tensorflow.keras
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import ShuffleSplit
from tensorflow.keras.layers import ReLU
from tensorflow.keras.optimizers import Adam


from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler



import logging, os
# Ignore useless W0819 warnings generated by TensorFlow 2.0.  
# Hopefully can remove this ignore in the future.
# See https://github.com/tensorflow/tensorflow/issues/31308
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"


# -----------------------------------------------------------------------------


def loadXY():

    '''
    Can't figure out how to pass non-optimiziable variable to the bayes opt
    module. Just load them from pickle everytime in evaluate_network()
    '''
    unfolding_data = pd.read_pickle("../data/unfolding_data.pkl")

    X = np.zeros((251,15))
    Y = np.zeros((251,60))
    for row in range(251):
    
        X[row,:] = unfolding_data['Detector Response'][row]
        Y[row,:] = unfolding_data['Spectrum'][row]
    
    return X,Y

# -----------------------------------------------------------------------------


# Nicely formatted time string
def hms_string(sec_elapsed):
    
    h = int(sec_elapsed / (60 * 60))
    m = int((sec_elapsed % (60 * 60)) / 60)
    s = sec_elapsed % 60
    return "{}:{:>02}:{:>05.2f}".format(h, m, s)


# -----------------------------------------------------------------------------


def generate_model(neuronPct, neuronShrink, layerPct, X, Y):

    # We start with some percent of 500 starting neurons in the first hidden layer.
    neuronCount = int(neuronPct * 500)
    layerCount = int(layerPct*9) # parameter to tune number of layers, up to five layers
    

    # Construct neural network
    # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)
    model = Sequential()

    # So long as there would have been at least 15 neurons and fewer than 10
    # layers, create a new layer.
    layer = 0
    while neuronCount>60 and layer <= layerCount:
        # The first (0th) layer needs an input input_dim(neuronCount)
        if layer==0:
            model.add(Dense(neuronCount, 
                input_dim=X.shape[1], 
                activation="relu"))
        else:
            model.add(Dense(neuronCount, activation="relu"))

        layer += 1

        # Shrink neuron count for each layer
        neuronCount = neuronCount * neuronShrink


    model.add(Dense(Y.shape[1],activation='linear')) # Output, linear for regression

    return model


# -----------------------------------------------------------------------------


def evaluate_network(lr,neuronPct,neuronShrink, layerPct, batch):

    [X,Y] = loadXY()

    xscaler = MinMaxScaler() # create scaler
    xscaler.fit(X) # fit scaler on data
    X = xscaler.transform(X) # apply transform
    
    #yscaler = StandardScaler()
    yscaler = MinMaxScaler() # create scaler
    yscaler.fit(Y) # fit scaler on data
    Y = yscaler.transform(Y) # apply transform

    SPLITS = 3

    # Bootstrap
    boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=42)
    
    # Track progress
    mean_benchmark = []
    epochs_needed = []
    num = 0

    # Loop through samples
    for train, test in boot.split(X):
        
        start_time = time.time()
        num += 1

        # Split train and test
        xtrain = X[train]
        ytrain = Y[train]
               
        xtest = X[test]
        ytest = Y[test]

        model = generate_model(neuronPct, neuronShrink, layerPct,
                               xtrain, ytrain)
                
        model.compile(loss='mse', optimizer=Adam(lr=lr))
        
        # Setup early stopping in epochs
        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, 
                                patience=100, verbose=0,
                                mode='min', restore_best_weights=True)

        # Train on the bootstrap sample
        model.fit(xtrain,ytrain,validation_data=(xtest,ytest),
                  callbacks=[monitor],verbose=0,epochs=5000, batch_size=int(batch))

#        model.summary()

        epochs = monitor.stopped_epoch
        epochs_needed.append(epochs)

        # Predict on the out of boot (validation)
        pred = model.predict(xtest)
   
        # Measure this bootstrap's log loss
#        score = metrics.mean_squared_error(ytest, pred)

        score = metrics.mean_squared_error(yscaler.inverse_transform(ytest),
                                           yscaler.inverse_transform(pred))

        mean_benchmark.append(score)
        m1 = statistics.mean(mean_benchmark)
        m2 = statistics.mean(epochs_needed)
        mdev = statistics.pstdev(mean_benchmark)

        # Record this iteration
        time_took = time.time() - start_time
        print(f"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}")        
    tensorflow.keras.backend.clear_session()
    return (-m1)


from bayes_opt import BayesianOptimization

# Supress NaN warnings
import warnings
warnings.filterwarnings("ignore",category=RuntimeWarning)


# Bounded region of parameter space
pbounds = {'lr': (0.001, 0.01),
           'neuronPct': (0.01, 1.001),  # going a little above one will help round to 500
           'neuronShrink': (0.01, 1.001),
           'layerPct': (0, 1.1), # a little above 1 will help round to 4
           'batch':(10, 30)
          }

optimizer = BayesianOptimization(
    f=evaluate_network,
    pbounds=pbounds,
    verbose=2,  # verbose = 1 prints only when a maximum 
    # is observed, verbose = 0 is silent
    random_state=1,
)

start_time = time.time()
optimizer.maximize(init_points=20, n_iter=1000,)
time_took = time.time() - start_time

print(f"Total runtime: {hms_string(time_took)}")
print(optimizer.max)

'''
No Normalization, no monoenergetic data
{'target': -0.000622151443757936,
 'params': {'batch': 29.91255639559978,
 'layerPct': 0.6196185758847922, 
 'lr': 0.0028904412940205914,
 'neuronPct': 0.6924134544238929, 
 'neuronShrink': 0.49553715629833117}}
'''

'''
no normalization, with monoenergetic
{'target': -0.0007018781247868025,
 'params': {'batch': 29.789913406644896,
 'layerPct': 0.7315295801342848, 
 'lr': 0.001074795741052131,
 'neuronPct': 0.8099514064540988,
 'neuronShrink': 0.9541311182451854}}
'''



